{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1968a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242ab736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text paragraph you can write any text\n",
    "text = \"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. The history of natural language processing generally started in the 1950s, although work can be found from earlier periods.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35598b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and digits\n",
    "Text = re.sub('[^a-zA-Z]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec6450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbf33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence into words and remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(word_tokenize(sentence))\n",
    "words = [word.lower() for word in words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593fa632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequency\n",
    "word_freq = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68dfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence scores based on word frequency\n",
    "sentence_scores = {}\n",
    "for sentence in sentences:\n",
    "    for word in word_tokenize(sentence.lower()):\n",
    "        if word in word_freq:\n",
    "            if len(sentence.split(' ')) < 30:\n",
    "                if sentence not in sentence_scores:\n",
    "                    sentence_scores[sentence] = word_freq[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e458b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. The history of natural language processing generally started in the 1950s, although work can be found from earlier periods.\n"
     ]
    }
   ],
   "source": [
    "# Generate summary by selecting top 3 sentences with highest scores\n",
    "summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
